#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—ã–µ –æ—Ç—á–µ—Ç—ã.
"""

import os
import re
import json
import sys
from pathlib import Path
from statistics import mean, stdev
from datetime import datetime

def parse_results_sheet(file_path):
    """–ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª results_sheet –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
        
        results = {
            'fio': {},
            'pgbench': {}
        }
        
        # –ü–∞—Ä—Å–∏–Ω–≥ FIO —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        fio_pattern = r'(\d+)\s+(.+?)\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)'
        for match in re.finditer(fio_pattern, content):
            test_num, test_name, iops, bandwidth, latency = match.groups()
            results['fio'][test_name.strip()] = {
                'IOPS': float(iops),
                'Bandwidth': float(bandwidth),
                'Latency': float(latency)
            }
        
        # –ü–∞—Ä—Å–∏–Ω–≥ pgbench —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        tps_match = re.search(r'TPS.*?:\s*([\d.]+)', content)
        lat_avg_match = re.search(r'–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞:\s*([\d.]+)', content)
        lat_std_match = re.search(r'–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏:\s*([\d.]+)', content)
        transactions_match = re.search(r'–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π:\s*(\d+)', content)
        
        if tps_match:
            results['pgbench'] = {
                'TPS': float(tps_match.group(1)),
                'Latency_Avg': float(lat_avg_match.group(1)) if lat_avg_match else None,
                'Latency_Stddev': float(lat_std_match.group(1)) if lat_std_match else None,
                'Transactions': int(transactions_match.group(1)) if transactions_match else None
            }
        
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path}: {e}")
        return None

def aggregate_results(results_dir):
    """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π"""
    results_dir = Path(results_dir)
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ results_sheet
    iterations_data = {}
    
    # –ò—â–µ–º —Ñ–∞–π–ª—ã –Ω–∞–ø—Ä—è–º—É—é –∏ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
    all_result_files = list(results_dir.glob('**/results_sheet_*.txt'))
    
    if not all_result_files:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ results_sheet_*.txt")
        return None
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_result_files)} —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    
    for file in all_result_files:
        print(f"  ‚Ä¢ {file.relative_to(results_dir)}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏–∑ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –∏–∑ –∏–º–µ–Ω–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        iter_num = None
        
        # –ü–æ–ø—ã—Ç–∫–∞ 1: –∏–∑ –∏–º–µ–Ω–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (iter1, iter2, etc.)
        for parent in file.parents:
            iter_match = re.search(r'iter(\d+)', parent.name)
            if iter_match:
                iter_num = int(iter_match.group(1))
                break
        
        # –ü–æ–ø—ã—Ç–∫–∞ 2: –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç iter)
        if iter_num is None:
            iter_match = re.search(r'iter(\d+)', file.name)
            if iter_match:
                iter_num = int(iter_match.group(1))
        
        # –ü–æ–ø—ã—Ç–∫–∞ 3: –ø–æ timestamp (–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
        if iter_num is None:
            # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º timestamp –∫–∞–∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            timestamp_match = re.search(r'(\d{8}_\d{6})', file.name)
            if timestamp_match:
                # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ-–Ω–æ–º–µ—Ä –∏—Ç–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö—ç—à–∞ timestamp
                timestamp = timestamp_match.group(1)
                iter_num = hash(timestamp) % 1000  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö—ç—à –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –∏—Ç–µ—Ä–∞—Ü–∏—è 1
        if iter_num is None:
            iter_num = 1
        
        parsed = parse_results_sheet(file)
        if parsed:
            if iter_num not in iterations_data:
                iterations_data[iter_num] = []
            iterations_data[iter_num].append(parsed)
    
    if not iterations_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        return None
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏—è–º
    aggregated = {
        'fio': {},
        'pgbench': {},
        'iterations': sorted(iterations_data.keys()),
        'num_vms': len(iterations_data[list(iterations_data.keys())[0]])
    }
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è FIO
    all_fio_tests = set()
    for iter_results in iterations_data.values():
        for vm_result in iter_results:
            all_fio_tests.update(vm_result['fio'].keys())
    
    for test_name in all_fio_tests:
        metrics = {'IOPS': [], 'Bandwidth': [], 'Latency': []}
        
        for iter_results in iterations_data.values():
            for vm_result in iter_results:
                if test_name in vm_result['fio']:
                    for metric in metrics.keys():
                        metrics[metric].append(vm_result['fio'][test_name][metric])
        
        aggregated['fio'][test_name] = {
            'IOPS_mean': mean(metrics['IOPS']),
            'IOPS_stdev': stdev(metrics['IOPS']) if len(metrics['IOPS']) > 1 else 0,
            'Bandwidth_mean': mean(metrics['Bandwidth']),
            'Bandwidth_stdev': stdev(metrics['Bandwidth']) if len(metrics['Bandwidth']) > 1 else 0,
            'Latency_mean': mean(metrics['Latency']),
            'Latency_stdev': stdev(metrics['Latency']) if len(metrics['Latency']) > 1 else 0,
            'samples': len(metrics['IOPS'])
        }
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è pgbench
    pgbench_metrics = {'TPS': [], 'Latency_Avg': [], 'Latency_Stddev': [], 'Transactions': []}
    
    for iter_results in iterations_data.values():
        for vm_result in iter_results:
            if vm_result['pgbench']:
                for metric, values in pgbench_metrics.items():
                    val = vm_result['pgbench'].get(metric)
                    if val is not None:
                        values.append(val)
    
    if pgbench_metrics['TPS']:
        aggregated['pgbench'] = {
            'TPS_mean': mean(pgbench_metrics['TPS']),
            'TPS_stdev': stdev(pgbench_metrics['TPS']) if len(pgbench_metrics['TPS']) > 1 else 0,
            'Latency_Avg_mean': mean(pgbench_metrics['Latency_Avg']),
            'Latency_Avg_stdev': stdev(pgbench_metrics['Latency_Avg']) if len(pgbench_metrics['Latency_Avg']) > 1 else 0,
            'samples': len(pgbench_metrics['TPS'])
        }
    else:
        print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ pgbench –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏")
    
    return aggregated

def generate_report(aggregated, output_file):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç"""
    report = []
    report.append("="*80)
    report.append("–ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    report.append("="*80)
    report.append(f"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {len(aggregated['iterations'])}")
    report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –í–ú: {aggregated['num_vms']}")
    report.append("")
    
    # FIO —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if aggregated['fio']:
        report.append("="*80)
        report.append("FIO - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–æ–≤–æ–π –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã (—Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)")
        report.append("="*80)
        report.append("")
        report.append(f"{'Test Name':<30} {'IOPS':<20} {'Bandwidth (MiB/s)':<20} {'Latency (ms)':<20}")
        report.append("-"*80)
        
        for test_name, metrics in sorted(aggregated['fio'].items()):
            report.append(
                f"{test_name:<30} "
                f"{metrics['IOPS_mean']:>8.1f} ¬±{metrics['IOPS_stdev']:>6.1f}  "
                f"{metrics['Bandwidth_mean']:>8.1f} ¬±{metrics['Bandwidth_stdev']:>6.1f}  "
                f"{metrics['Latency_mean']:>8.2f} ¬±{metrics['Latency_stdev']:>6.2f}"
            )
        report.append("")
    
    # pgbench —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if aggregated['pgbench']:
        report.append("="*80)
        report.append("pgbench - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ PostgreSQL OLTP (—Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)")
        report.append("="*80)
        report.append("")
        pg = aggregated['pgbench']
        report.append(f"TPS (Transactions Per Second): {pg['TPS_mean']:.2f} ¬± {pg['TPS_stdev']:.2f}")
        report.append(f"–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {pg['Latency_Avg_mean']:.3f} ¬± {pg['Latency_Avg_stdev']:.3f} ms")
        report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π: {pg['samples']}")
        report.append("")
    else:
        report.append("="*80)
        report.append("pgbench - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ PostgreSQL OLTP")
        report.append("="*80)
        report.append("")
        report.append("‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã pgbench –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç (—Ç–µ—Å—Ç –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª—Å—è –∏–ª–∏ –Ω–µ –±—ã–ª –≤–∫–ª—é—á–µ–Ω)")
        report.append("")
    
    report.append("="*80)
    report.append("–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ó–Ω–∞—á–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ '—Å—Ä–µ–¥–Ω–µ–µ ¬± —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ'")
    report.append("="*80)
    
    report_text = "\n".join(report)
    
    # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    print(report_text)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    with open(output_file, 'w') as f:
        f.write(report_text)
    
    print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    
    return report_text

def save_json(aggregated, output_file):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON"""
    with open(output_file, 'w') as f:
        json.dump(aggregated, f, indent=2)
    print(f"üìä JSON –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 aggregate_results.py <–ø—É—Ç—å_–∫_–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏_—Å_—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏>")
        sys.exit(1)
    
    results_dir = sys.argv[1]
    
    if not os.path.exists(results_dir):
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {results_dir}")
        sys.exit(1)
    
    print(f"üìÅ –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤: {results_dir}")
    print("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    aggregated = aggregate_results(results_dir)
    
    if not aggregated:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        sys.exit(1)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
    output_base = os.path.join(results_dir, "aggregated_report")
    generate_report(aggregated, f"{output_base}.txt")
    save_json(aggregated, f"{output_base}.json")
    
    print("\n‚úÖ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main()